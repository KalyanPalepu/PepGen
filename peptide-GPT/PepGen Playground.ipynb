{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e26c86de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "83813c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b1dcc07f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "binding_regions.csv binding_regions.py\r\n"
     ]
    }
   ],
   "source": [
    "!ls ../../data/propedia/parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e211521f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9555e4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7609e5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = ast.literal_eval(regions.iloc[0]['binding_region_indices'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1591fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "regions = pd.read_csv(\"../../data/propedia/parsing/binding_regions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74bd9189",
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths = regions.peptide_seq.str.len() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d2fc899f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9968171145316984"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lengths[lengths < 50]) / len(lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6c63e62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "err_row = regions[regions['propedia_id'] == '6ghl_E_A']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "13e3c66a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8433    338\n",
       "Name: partner_seq, dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "err_row['partner_seq'].str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7ad1e1a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8433    21\n",
       "Name: peptide_seq, dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "err_row['peptide_seq'].str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "46bc3f1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8433    359\n",
       "dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "err_row['partner_seq'].str.len() + err_row['peptide_seq'].str.len()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0fead757",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
       "        [False, False,  True,  True,  True,  True,  True,  True,  True,  True],\n",
       "        [False, False, False,  True,  True,  True,  True,  True,  True,  True],\n",
       "        [False, False, False, False,  True,  True,  True,  True,  True,  True],\n",
       "        [False, False, False, False, False,  True,  True,  True,  True,  True],\n",
       "        [False, False, False, False, False, False,  True,  True,  True,  True],\n",
       "        [False, False, False, False, False, False, False,  True,  True,  True],\n",
       "        [False, False, False, False, False, False, False, False,  True,  True],\n",
       "        [False, False, False, False, False, False, False, False, False,  True],\n",
       "        [False, False, False, False, False, False, False, False, False, False]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "704ff729",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9997391077484998"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lengths = regions['partner_seq'].str.len() + 50\n",
    "len(lengths[lengths < 1020]) / len(lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59f54de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "47913cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "a346a8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import esm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "id": "877ae910",
   "metadata": {},
   "outputs": [],
   "source": [
    "esm_dim = 768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "id": "eabbd550",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_esm = torch.tensor(np.load(\"DLX5_embedding.npy\"))[:, :, :768]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "1bbce702",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding():\n",
    "    def __init__(self, dim=esm_dim, max_seq_length=1024):        \n",
    "        # positional embeddings as defined in Attention is All You Need\n",
    "        positions = torch.arange(max_peptide, dtype=torch.float)\n",
    "        freqs = 1 / torch.pow(10000, torch.arange(0, dim, 2) / dim)\n",
    "\n",
    "        self.pos_embeddings = torch.zeros(max_peptide, dim)\n",
    "        trig_arguments = torch.matmul(positions.unsqueeze(-1), torch.t(freqs.unsqueeze(-1)))\n",
    "        self.pos_embeddings[:, torch.arange(0, dim, 2)] = torch.sin(trig_arguments)\n",
    "        self.pos_embeddings[:, torch.arange(1, dim, 2)] = torch.cos(trig_arguments)\n",
    "    \n",
    "    def __call__(self, size):\n",
    "        return self.pos_embeddings[:size, :]\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "id": "8a636c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_alphabet():\n",
    "    alphabet = esm.Alphabet.from_architecture(\"msa_transformer\")\n",
    "    alphabet.prepend_bos = False\n",
    "    alphabet.append_eos = True\n",
    "    alphabet.use_msa = False\n",
    "    return alphabet\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "id": "e092c100",
   "metadata": {},
   "outputs": [],
   "source": [
    "alphabet = get_alphabet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "id": "83459eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: make pad, unk all -100 in dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "id": "4f25fa12",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"esm_dim\": esm_dim,\n",
    "    \"alphabet\": get_alphabet(),\n",
    "    \"lr\": 1e-3,\n",
    "    \"hidden_dim\": 40, # dim used in transformer decoder\n",
    "    \"num_heads\": 4, \n",
    "    \"num_layers\": 2,\n",
    "    \"dim_feedforward\": 80, # d_model * 2?\n",
    "    \"layer_norm_eps\": 1e-5,\n",
    "    \"dropout\": 0.1,\n",
    "    \"batch_first\": True,\n",
    "    \"norm_first\": True,\n",
    "    \"use_esm_token_embedding\": True,\n",
    "    \"esm_token_embedding_path\": \"esm_token_embedding.pt\",\n",
    "    \"positional_embed_peptide_only\": True\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 684,
   "id": "38204480",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8])"
      ]
     },
     "execution_count": 684,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_input['peptide_labels'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "id": "8b4df24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input = {\n",
    "    'peptide_input': torch.tensor([2, 4, 4, 5, 3, 0, 0, 0]).reshape(1, -1),\n",
    "    'partner_embedding': test_esm,\n",
    "    'peptide_labels': torch.tensor([2, 4, -100, -100, 3, 0, 0, 0]).reshape(1, -1),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "id": "6d02341d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pe = PeptideEmbedding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 682,
   "id": "b31776cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import pytorch_lightning as pl\n",
    "import pickle\n",
    "from math import floor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 690,
   "id": "f339f021",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class PepGenGPT(pl.LightningModule):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        self.sep_idx = self.config[\"alphabet\"].get_idx(\"<sep>\")\n",
    "        self.eos_idx = self.config[\"alphabet\"].get_idx(\"<eos>\")\n",
    "        self.pad_idx = self.config[\"alphabet\"].get_idx(\"<pad>\")\n",
    "        \n",
    "        if self.config['use_esm_token_embedding']:\n",
    "            self.peptide_embedder = nn.Embedding(\n",
    "                len(config['alphabet']),\n",
    "                self.config['esm_dim'],\n",
    "                padding_idx=self.pad_idx\n",
    "            )\n",
    "            self.peptide_embedder.load_state_dict(torch.load(self.config[\"esm_token_embedding_path\"]))\n",
    "            \n",
    "            # load esm embedder\n",
    "            self.peptide_embedding_map = nn.Linear(\n",
    "                self.config['esm_dim'], \n",
    "                self.config['hidden_dim']\n",
    "            )\n",
    "        else:\n",
    "            self.peptide_embedder = nn.Embedding(\n",
    "                len(config['alphabet']),\n",
    "                self.config['hidden_dim'],\n",
    "                padding_idx=self.pad_idx\n",
    "            )\n",
    "        \n",
    "        # reduce dimensionality of input embedding\n",
    "        self.esm_embedding_map = nn.Linear(self.config['esm_dim'], self.config['hidden_dim'])\n",
    "        \n",
    "        self.positional_embedding = PositionalEmbedding(dim=self.config['hidden_dim'])\n",
    "        \n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=self.config['hidden_dim'],\n",
    "            nhead=self.config['num_heads'],\n",
    "            dim_feedforward=self.config['dim_feedforward'],\n",
    "            dropout=self.config['dropout'],\n",
    "            layer_norm_eps=self.config['layer_norm_eps'],\n",
    "            batch_first=self.config['batch_first'],\n",
    "            norm_first=self.config[\"norm_first\"]\n",
    "        )\n",
    "        output_norm = nn.LayerNorm(self.config['hidden_dim'], eps=self.config['layer_norm_eps'])\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, self.config['num_layers'], output_norm)\n",
    "        self.logit_map = nn.Linear(self.config['hidden_dim'], len(config['alphabet']))\n",
    "    \n",
    "    def embed_peptide(self, peptide_input):\n",
    "        embedding = self.peptide_embedder(peptide_input)\n",
    "        if self.config['use_esm_token_embedding']:\n",
    "            embedding = self.peptide_embedding_map(embedding)\n",
    "        \n",
    "        return embedding\n",
    "    \n",
    "    def get_causal_mask(self, partner_length, peptide_length):\n",
    "        total_length = partner_length + peptide_length\n",
    "        mask = torch.ones(total_length, total_length, dtype=torch.bool)\n",
    "        mask[:, :partner_length] = False\n",
    "        mask[-peptide_length:, -peptide_length:] = ~torch.tril(torch.ones(peptide_length, peptide_length, dtype=torch.bool))\n",
    "    \n",
    "    def forward(self, peptide_input, raw_partner_embedding, padding_mask):\n",
    "        peptide_length = peptide_input.shape[1]\n",
    "        \n",
    "        peptide_embedding = self.embed_peptide(peptide_input)\n",
    "        partner_embedding = self.esm_embedding_map(raw_partner_embedding)\n",
    "        \n",
    "        x = torch.cat([partner_embedding, peptide_embedding], dim=1)\n",
    "\n",
    "        partner_length = x.shape[1] - peptide_length\n",
    "        if self.config['positional_embed_peptide_only']:\n",
    "            x[:, -peptide_length:, :] += self.positional_embedding(peptide_length).unsqueeze(0)\n",
    "        else:\n",
    "            x += self.positional_embedding(x.shape[1]).unsqueeze(0)\n",
    "        \n",
    "        causal_mask = self.get_causal_mask(partner_length, peptide_length)\n",
    "        \n",
    "        x = self.encoder(\n",
    "            src=x,\n",
    "            mask=causal_mask,\n",
    "            src_key_padding_mask=padding_mask,\n",
    "        )\n",
    "        \n",
    "        x = self.logit_map(x)\n",
    "        \n",
    "        # we only care about the peptide predictions\n",
    "        x = x[:, -peptide_length:, :]\n",
    "        return x\n",
    "            \n",
    "    def predict(self, partner_embedding, max_length=50):        \n",
    "        partner_embedding = self.esm_embedding_map(partner_embedding)\n",
    "        peptide_seq = torch.tensor([self.sep_idx]).reshape(1, -1)\n",
    "        \n",
    "        for i in range(max_length):\n",
    "            total_length = partner_embedding.shape[1] + peptide_seq.shape[1]\n",
    "            padding_mask = torch.zeros(1, total_length, dtype=torch.bool)\n",
    "            logits = self(peptide_seq, partner_embedding, padding_mask)\n",
    "            next_token = logits.topk(1, dim=2).indices[0, -1, 0]\n",
    "            peptide_seq = torch.cat([peptide_seq, next_token.reshape(1, -1)], dim=1)\n",
    "            if next_token.item() == self.eos_idx:\n",
    "                break\n",
    "\n",
    "        return peptide_seq\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        logits = self(batch['peptide_input'], batch['partner_embedding'], batch['padding_mask'])\n",
    "                \n",
    "        # B x L x C -> B x C x L\n",
    "        # permute to agree with cross-entropy dims\n",
    "        logits = logits.permute(0, 2, 1)\n",
    "        \n",
    "        loss = F.cross_entropy(logits, batch['peptide_labels'])\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx: int) -> None:\n",
    "        logits = self(batch['peptide_input'], batch['partner_embedding'], batch['padding_mask'])\n",
    "\n",
    "        # B x L x C -> B x C x L\n",
    "        # permute to agree with cross-entropy dims\n",
    "        logits = logits.permute(0, 2, 1)\n",
    "        pred = logits.argmax(dim=1, keepdim=True)\n",
    "\n",
    "        loss = F.cross_entropy(logits, batch['peptide_labels'])\n",
    "\n",
    "        accuracy = pred.eq(target.view_as(pred)).float().mean()\n",
    "        self.log(\"val_acc\", accuracy)\n",
    "        self.log(\"hp_metric\", accuracy, on_step=False, on_epoch=True)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.config['lr'])\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "id": "e40c4878",
   "metadata": {},
   "outputs": [],
   "source": [
    "regions = pd.read_csv(\"../../parsing/propedia/binding_regions.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "561a0cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "propedia_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "id": "c91d3828",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestEmbeddingDict():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def __getitem__(self, index):\n",
    "        return test_esm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 674,
   "id": "3334a0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PartnerPeptideDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataframe, partner_embedding_dict):\n",
    "        self.dataframe = dataframe[dataframe[\"propedia_id\"].isin(partner_embedding_dict.keys())]\n",
    "        self.dataframe = self.dataframe.reset_index()\n",
    "        self.partner_embedding_dict = partner_embedding_dict\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Select row\n",
    "        row = self.dataframe.iloc[index]\n",
    "        propedia_id = row[\"propedia_id\"]\n",
    "        \n",
    "        return {\n",
    "            \"propedia_id\": propedia_id,\n",
    "            \"peptide_seq\": row[\"peptide_seq\"],\n",
    "            \"partner_embedding\": self.partner_embedding_dict[propedia_id]\n",
    "        } "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 675,
   "id": "0da27ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dict = {\"3p3n_B_A\": test_esm, \"3si4_I_H\": test_esm}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 676,
   "id": "b3cca4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = PartnerPeptideDataset(regions, test_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 677,
   "id": "9ac9c953",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PartnerPeptideCollator:\n",
    "    def __init__(self, alphabet):\n",
    "        self.alphabet = alphabet\n",
    "        self.batch_converter = alphabet.get_batch_converter()\n",
    "        self.sep_idx = self.alphabet.get_idx(\"<sep>\")\n",
    "        self.eos_idx = self.alphabet.get_idx(\"<eos>\")\n",
    "        self.pad_idx = self.alphabet.get_idx(\"<pad>\")\n",
    "    \n",
    "    def __call__(self, raw_batch):\n",
    "        n_seq = len(raw_batch)\n",
    "        batch = {}\n",
    "        _, _, peptide_tokens = self.batch_converter([(d['propedia_id'], d['peptide_seq']) for d in raw_batch])\n",
    "        peptide_tokens = torch.cat([torch.full((n_seq, 1), self.sep_idx), peptide_tokens], dim=1)\n",
    "        batch['peptide_input'] = peptide_tokens[:, :-1]\n",
    "        batch['peptide_labels'] = peptide_tokens[:, 1:]\n",
    "        \n",
    "        batch['partner_embedding'] = pad_sequence(\n",
    "            [d['partner_embedding'][0, :, :] for d in raw_batch],\n",
    "            batch_first=True,\n",
    "            padding_value=0\n",
    "        )\n",
    "        \n",
    "        partner_raw_lengths = [d['partner_embedding'].shape[1] for d in raw_batch]\n",
    "         \n",
    "        batch['padding_mask'] = self.get_padding_mask(\n",
    "            batch['partner_embedding'].shape[1],\n",
    "            partner_raw_lengths,\n",
    "            batch['peptide_input']\n",
    "        )\n",
    "        return batch\n",
    "        \n",
    "    def get_padding_mask(self, partner_batch_length, partner_raw_lengths, peptide_input):\n",
    "        batch_size = peptide_input.shape[0]\n",
    "        peptide_length = peptide_input.shape[1]\n",
    "        total_length = partner_length + peptide_length\n",
    "        \n",
    "        print(peptide_input.shape)\n",
    "        \n",
    "        mask = torch.ones(batch_size, total_length, dtype=torch.bool)\n",
    "        \n",
    "        mask[:, -peptide_length:] = peptide_input == self.pad_idx\n",
    "        for i, length in enumerate(partner_raw_lengths):\n",
    "            mask[i, :length] = False\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 683,
   "id": "413671c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PartnerPeptideDataModule(pl.LightningDataModule):\n",
    "    def __init__(self,\n",
    "                 regions_csv_path,\n",
    "                 partner_embeddings_path,\n",
    "                 train_frac=0.8,\n",
    "                 test_frac=0.1,\n",
    "                 val_frac=0.1,\n",
    "                 batch_size=4,\n",
    "                 random_seed=42):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.regions_csv_path = regions_csv_path\n",
    "        self.partner_embeddings_path = partner_embeddings_path\n",
    "        self.train_frac = train_frac\n",
    "        self.test_frac = test_frac\n",
    "        self.val_frac = val_frac\n",
    "        self.batch_size = batch_size\n",
    "        self.random_seed = random_seed\n",
    "\n",
    "    def setup(self, stage):\n",
    "        regions = pd.read_csv(self.regions_csv_path)\n",
    "        embeddings = pickle.load(self.partner_embeddings_path)\n",
    "        \n",
    "        raw_dataset = PartnerPeptideDataset(regions, embeddings)\n",
    "        \n",
    "        \n",
    "        train_length = floor(train_frac * len(raw_dataset))\n",
    "        test_length = floor(test_frac * len(raw_dataset))\n",
    "        val_length = len(raw_dataset) - train_length - test_length\n",
    "        \n",
    "        splits = random_split(raw_dataset, \n",
    "                              [train_length, test_length, val_length], \n",
    "                              generator=torch.Generator().manual_seed(self.random_seed))\n",
    "        \n",
    "        self.train_dataset, self.test_dataset, self.val_dataset = splits\n",
    "        \n",
    "        self.alphabet = self.get_alphabet()\n",
    "        self.collator = PartnerPeptideCollator(alphabet)\n",
    "        \n",
    "    def get_alphabet(self):\n",
    "        alphabet = esm.Alphabet.from_architecture(\"msa_transformer\")\n",
    "        alphabet.prepend_bos = False\n",
    "        alphabet.append_eos = True\n",
    "        alphabet.use_msa = False\n",
    "        return alphabet\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size, collate_fn=self.collator)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_dataset, batch_size=self.batch_size, collate_fn=self.collator)\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_dataset, batch_size=self.batch_size, collate_fn=self.collator)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 691,
   "id": "057bafc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "collator = PartnerPeptideCollator(alphabet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 692,
   "id": "7b74193c",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt = PepGenGPT(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 693,
   "id": "085f0e68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 12])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(3.7980, grad_fn=<NllLoss2DBackward0>)"
      ]
     },
     "execution_count": 693,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt.training_step(collator(test_raw_batch), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 694,
   "id": "a2e179d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 12])\n"
     ]
    }
   ],
   "source": [
    "batch = collator(test_raw_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 695,
   "id": "a57d908d",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = gpt(batch['peptide_input'], batch['partner_embedding'], batch['padding_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 697,
   "id": "caaf202a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 12, 33])"
      ]
     },
     "execution_count": 697,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 698,
   "id": "326cfddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = logits.permute(0, 2, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 704,
   "id": "359cfcaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = logits.argmax(dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 705,
   "id": "8de44a0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 12])"
      ]
     },
     "execution_count": 705,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 706,
   "id": "72cac741",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 12])"
      ]
     },
     "execution_count": 706,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 708,
   "id": "03e7a09c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[28, 28, 28, 28, 21, 21, 28, 28, 28, 28, 23, 21],\n",
       "        [28, 12, 28, 28, 23, 28, 28, 28, 28, 28, 21, 28]])"
      ]
     },
     "execution_count": 708,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 709,
   "id": "6a9e9441",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[10,  4,  4,  9,  5,  8,  5, 13,  5, 17,  2,  1],\n",
       "        [13, 18,  9,  9, 12, 14,  9,  9, 24,  4, 16,  2]])"
      ]
     },
     "execution_count": 709,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['peptide_labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 707,
   "id": "3d944230",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 707,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred.eq(batch['peptide_labels']).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4460eb75",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
